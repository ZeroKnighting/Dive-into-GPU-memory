{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vryeDpr11asv"
      },
      "source": [
        "先定义一个4b的tensor，并向显存申请4b的空间，我们可以看见其分配了512b的空间，同时pytorch向GPU申请了2MB的空间存储在cache中"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXzqpulHrgBC",
        "outputId": "9732a0cd-cf20-4e2e-e1cc-d3be39305996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512 B\n",
            "2097152 B\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "a = torch.zeros(1).to('cuda')\n",
        "print(torch.cuda.memory_allocated(),'B')\n",
        "print(torch.cuda.memory_reserved(),'B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQlsu-TE10mV"
      },
      "source": [
        "删除a，再次检测显存占用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-la6Sxa1sHD1"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8u6ZgSVtnlc",
        "outputId": "ab39aa1f-898f-49f6-ae89-cfc4e6373699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 B\n",
            "2097152 B\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_allocated(),'B')\n",
        "print(torch.cuda.memory_reserved(),'B')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbR9aEbc14wg"
      },
      "source": [
        "清空pytroch的cache，这种行为仅仅建议当你想要释放缓存以便让其他人也可以一起使用当前显卡，否则不需要调用这个方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5da4VyUtZKQ",
        "outputId": "5faa6c32-6636-44e6-ea4b-08454d5e15a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 B\n",
            "0 B\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_allocated(),'B')\n",
        "print(torch.cuda.memory_reserved(),'B')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eB4HEa6S2Eio"
      },
      "source": [
        "再看另外一个例子，这次向显存申请10240\\*1050\\*4/1024/1024=41.015625MB的显存空间，同理可见pytorch为其分配了42MB的显存空间"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx2zxasC1EEy",
        "outputId": "61192827-df0c-4d81-fc39-d3e049def8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42.0 MB\n",
            "42.0 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "a = torch.zeros(10240,1050).to('cuda')\n",
        "print(torch.cuda.memory_allocated()/1024/1024,'MB')\n",
        "print(torch.cuda.memory_reserved()/1024/1024,'MB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Oppfinj1PsS"
      },
      "outputs": [],
      "source": [
        "del a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzfLaGbt1RzH"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IISVATKG2vwr"
      },
      "source": [
        "我们来实际看一下这种情况对于模型训练的分配情况的影响（此处仅仅介绍缓存按页分配，不分析其他的计算，关于模型GPU显存的占用请看下一章节）\n",
        "由下方代码可知，最后为模型结果以及中间变量分配了8.515625MB的显存空间，但是其实际占用仅仅为10240\\*10\\*4/1024/1024=0.390625MB，表明理解显存按页分配对模型GPU占用具有重要意义"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_FxQzsu2xXA",
        "outputId": "725305b1-83b0-4876-a5ba-d991e9594edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mymodel:  (0.03955078125, 0.03955078125)\n",
            "input:  (40.03955078125, 40.0)\n",
            "output and intermediate:  (48.55517578125, 8.515625)\n",
            "62.0 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "last_gpu_memory = 0\n",
        "\n",
        "def get_memory():\n",
        "  global last_gpu_memory\n",
        "  last = last_gpu_memory\n",
        "  now = torch.cuda.memory_allocated()/1024/1024\n",
        "  last_gpu_memory = now\n",
        "  return now,now-last\n",
        "class BasicModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.lr = torch.nn.Linear(1024,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.lr(x)\n",
        "    return x\n",
        "\n",
        "model = BasicModel().to('cuda')\n",
        "print('mymodel: ',get_memory())\n",
        "data = torch.zeros(10240,1024).to('cuda')\n",
        "print('input: ',get_memory())\n",
        "out = model(data)\n",
        "print(\"output and intermediate: \",get_memory())\n",
        "print(torch.cuda.memory_reserved()/1024/1024,'MB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTU91W9FWaM"
      },
      "source": [
        "使用pynvml 库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUB-b7KaFaKv",
        "outputId": "9e04634d-a0a6-41b1-9df5-655b664a7f75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCO8JchFmD4"
      },
      "source": [
        "查看占用的缓存，在使用之前先检测kernal占用的gpu大小（查看之前先清空之前pytorch申请的内存）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5HN8hQVFwQE",
        "outputId": "10b22b14-0db9-430d-a869-cefcb1ca4bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory occupied: 258 MB.\n",
            "GPU memory occupied: 363 MB.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from pynvml import *\n",
        "\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "\n",
        "print_gpu_utilization()\n",
        "\n",
        "torch.ones((1, 1)).to(\"cuda\")\n",
        "print_gpu_utilization()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
