{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "定义输入，x和y大小都为1GB"
      ],
      "metadata": {
        "id": "LiDyC35waFaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor_size = 256 * 1024 * 1024\n",
        "x = torch.randn(tensor_size, dtype=torch.float32, device='cuda')\n",
        "y = torch.randn(tensor_size, dtype=torch.float32, device='cuda')\n"
      ],
      "metadata": {
        "id": "892JNeKQKE2p"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "初始化"
      ],
      "metadata": {
        "id": "FzvXZvuJaLv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "current_memory = torch.cuda.memory_allocated()\n",
        "torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "id": "veJQJ9kMKXYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面的一些算子用来分析不同算子的显存占用的影响\n",
        "第一个算子是不保存梯度的简单的算术算子"
      ],
      "metadata": {
        "id": "Sq99_kZYaM8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(x, y):\n",
        "    return (x + 1) * (2 * y)"
      ],
      "metadata": {
        "id": "mr8dFPWaKOV4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果x和y都需要保存梯度"
      ],
      "metadata": {
        "id": "yWSAROsWabvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(x, y):\n",
        "    x.requires_grad_(True)\n",
        "    y.requires_grad_(True)\n",
        "    return (x + 1) * (2 * y)"
      ],
      "metadata": {
        "id": "EmqysrqSX4gE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果只保存x的梯度"
      ],
      "metadata": {
        "id": "xfEetjXfaeQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(x, y):\n",
        "    x.requires_grad_(True)\n",
        "    return (x + 1) * (2 * y)"
      ],
      "metadata": {
        "id": "RgniHHM0xBgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "算子融合，手动计算梯度，可以降低持续显存占用"
      ],
      "metadata": {
        "id": "4zUXScAfagmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "class AddMulFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        z = (x + 1) * (2 * y)\n",
        "        #print(z.requires_grad)\n",
        "        #print(z.grad_fn)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, y = ctx.saved_tensors\n",
        "        grad_x = grad_output * (2 * y)\n",
        "        grad_y = grad_output * (x + 1)\n",
        "        return grad_x, grad_y\n",
        "\n",
        "func = AddMulFunction.apply\n",
        "\n",
        "def compute(x, y):\n",
        "    x.requires_grad_(True)\n",
        "    y.requires_grad_(True)\n",
        "    return func(x, y)\n",
        "\n",
        "#print(z.requires_grad)\n",
        "#print(z.grad_fn)"
      ],
      "metadata": {
        "id": "rcJ-kZcAz00b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "继续优化算子，降低其峰值显存占用"
      ],
      "metadata": {
        "id": "z8vAIBYGamJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Function\n",
        "class AddMulFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y):\n",
        "        ctx.save_for_backward(x, y)\n",
        "        z = x+1\n",
        "        z = z*2\n",
        "        z = z*y\n",
        "        #print(z.requires_grad)\n",
        "        #print(z.grad_fn)\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, y = ctx.saved_tensors\n",
        "        grad_x = grad_output * (2 * y)\n",
        "        grad_y = grad_output * (x + 1)\n",
        "        return grad_x, grad_y\n",
        "\n",
        "func = AddMulFunction.apply\n",
        "\n",
        "def compute(x, y):\n",
        "    x.requires_grad_(True)\n",
        "    y.requires_grad_(True)\n",
        "    return func(x, y)\n",
        "\n",
        "#print(z.requires_grad)\n",
        "#print(z.grad_fn)"
      ],
      "metadata": {
        "id": "tazCGQoaYDl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "计算输出"
      ],
      "metadata": {
        "id": "6-8v9DtBao62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = compute(x, y)"
      ],
      "metadata": {
        "id": "lciKKt82ZMnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "自定义的sigmoid算子，其效率较差，显存占用较高"
      ],
      "metadata": {
        "id": "lhDzcxXpaqso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(x):\n",
        "    x.requires_grad_(True)\n",
        "    z = 1 / (1 + torch.exp(-x))\n",
        "    return z"
      ],
      "metadata": {
        "id": "QD3-iQtCWpjD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch提供的sigmoid算子，经过了算子融合等优化"
      ],
      "metadata": {
        "id": "ZQpKzml3av9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(x):\n",
        "    x.requires_grad_(True)\n",
        "    z = torch.nn.Sigmoid()(x)\n",
        "    return z"
      ],
      "metadata": {
        "id": "ftt4yCyJW_xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "计算输出"
      ],
      "metadata": {
        "id": "_R-4bnv2a0ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = compute(x)"
      ],
      "metadata": {
        "id": "idaZe_u7KaGR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "统计最后的占用:\n",
        "- 持续显存占用\n",
        "- 峰值显存占用"
      ],
      "metadata": {
        "id": "BJ-B26Mja3Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def memory():\n",
        "  additional_memory = torch.cuda.memory_allocated() - (current_memory + 1e9)\n",
        "  peak_memory = torch.cuda.max_memory_allocated()\n",
        "  additional_peak_memory = peak_memory - (current_memory + 1e9)\n",
        "\n",
        "  print(f\"Additional memory used: {additional_memory / (1024 ** 3)} GB\")\n",
        "  print(f\"Additional peak memory used: {additional_peak_memory / (1024 ** 3)} GB\")\n",
        "\n",
        "memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKFzSaGNKQtv",
        "outputId": "d172d5af-f4a7-493a-fbc6-e14e12e914b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Additional memory used: 2.0686774253845215 GB\n",
            "Additional peak memory used: 3.0686774253845215 GB\n"
          ]
        }
      ]
    }
  ]
}